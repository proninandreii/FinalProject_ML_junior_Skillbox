{ "cells": [  {   "cell_type": "code",   "execution_count": 6,   "id": "initial_id",   "metadata": {    "collapsed": true,    "ExecuteTime": {     "end_time": "2023-12-22T08:36:33.140889Z",     "start_time": "2023-12-22T08:36:33.137975Z"    }   },   "outputs": [],   "source": [    "import pyarrow.parquet as pq\n",    "import pandas as pd\n",    "import matplotlib.pyplot as plt\n",    "from sklearn.preprocessing import StandardScaler\n",    "import os\n",    "import pandas as pd\n",    "import tqdm\n",    "import pyarrow as pa\n",    "\n",    "import warnings\n",    "warnings.filterwarnings('ignore')\n",    "\n",    "plt.style.use('fivethirtyeight')"   ]  },  {   "cell_type": "code",   "execution_count": 7,   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "Обработка файла завершена.\n"     ]    }   ],   "source": [    "# Задаем данные\n",    "target_path = \"train_target.csv\"\n",    "path = 'train_data'\n",    "\n",    "target_df = pd.read_csv(target_path)\n",    "\n",    "table = pq.read_table(path)\n",    "df = table.to_pandas()\n",    "\n",    "print(f\"Обработка файла завершена.\")"   ],   "metadata": {    "collapsed": false,    "ExecuteTime": {     "end_time": "2023-12-22T08:36:40.203130Z",     "start_time": "2023-12-22T08:36:33.142489Z"    }   },   "id": "9bf6f0b5cb7a5323"  },  {   "cell_type": "code",   "execution_count": 8,   "outputs": [],   "source": [    "def read_parquet_dataset_from_local(path_to_dataset: str, start_from: int = 0,\n",    "                                    num_parts_to_read: int = 2, columns=None, verbose=False) -> pd.DataFrame:\n",    "    \"\"\"\n",    "    читает num_parts_to_read партиций, преобразовывает их к pd.DataFrame и возвращает\n",    "    :param path_to_dataset: путь до директории с партициями\n",    "    :param start_from: номер партиции, с которой нужно начать чтение\n",    "    :param num_parts_to_read: количество партиций, которые требуется прочитать\n",    "    :param columns: список колонок, которые нужно прочитать из партиции\n",    "    :return: pd.DataFrame\n",    "    \"\"\"\n",    "\n",    "    res = []\n",    "    dataset_paths = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset)\n",    "                            if filename.startswith('train')])\n",    "    print(dataset_paths)\n",    "\n",    "    start_from = max(0, start_from)\n",    "    chunks = dataset_paths[start_from: start_from + num_parts_to_read]\n",    "    if verbose:\n",    "        print('Reading chunks:\\n')\n",    "        for chunk in chunks:\n",    "            print(chunk)\n",    "    for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n",    "        print('chunk_path', chunk_path)\n",    "        chunk = pd.read_parquet(chunk_path,columns=columns)\n",    "        res.append(chunk)\n",    "\n",    "    return pd.concat(res).reset_index(drop=True)"   ],   "metadata": {    "collapsed": false,    "ExecuteTime": {     "end_time": "2023-12-22T08:36:40.205434Z",     "start_time": "2023-12-22T08:36:40.202955Z"    }   },   "id": "b17705c08b6be95b"  },  {   "cell_type": "code",   "execution_count": 9,   "outputs": [],   "source": [    "def prepare_transactions_dataset(path_to_dataset: str, num_parts_to_preprocess_at_once: int = 1, num_parts_total: int=50,\n",    "                                 save_to_path=None, verbose: bool=False):\n",    "    \"\"\"\n",    "    возвращает готовый pd.DataFrame с признаками, на которых можно учить модель для целевой задачи\n",    "    path_to_dataset: str\n",    "        путь до датасета с партициями\n",    "    num_parts_to_preprocess_at_once: int\n",    "        количество партиций, которые будут одновременно держаться и обрабатываться в памяти\n",    "    num_parts_total: int\n",    "        общее количество партиций, которые нужно обработать\n",    "    save_to_path: str\n",    "        путь до папки, в которой будет сохранён каждый обработанный блок в .parquet-формате; если None, то не будет сохранён\n",    "    verbose: bool\n",    "        логирует каждую обрабатываемую часть данных\n",    "    \"\"\"\n",    "    preprocessed_frames = []\n",    "\n",    "    for step in tqdm.tqdm_notebook(range(0, num_parts_total, num_parts_to_preprocess_at_once),\n",    "                                   desc=\"Transforming transactions data\"):\n",    "        df = read_parquet_dataset_from_local(path_to_dataset, step, num_parts_to_preprocess_at_once,\n",    "                                                             verbose=verbose)\n",    "\n",    "\n",    "        #здесь должен быть препроцессинг данных\n",    "        df['no_delinquencies'] = (df['is_zero_loans530'].astype(bool) & df['is_zero_loans3060'].astype(bool) &\n",    "                                  df['is_zero_loans90'].astype(bool) & df['is_zero_loans6090'].astype(bool) &\n",    "                                  df['is_zero_loans5'].astype(bool)).astype(int)\n",    "\n",    "        df['total_delinquencies'] = (df['is_zero_loans530'] + df['is_zero_loans3060'] + df['is_zero_loans5'] +\n",    "                                     df['is_zero_loans90'] + df['is_zero_loans6090'])\n",    "        \n",    "        enc_loans_columns = ['enc_loans_account_holder_type', 'enc_loans_credit_status', \n",    "                             'enc_loans_account_cur', 'enc_loans_credit_type', 'rn', 'id']\n",    "        filtered_columns = [col for col in df.columns if col.startswith('enc_paym_')]\n",    "        \n",    "        # Создание нового датафрейма с выбранными колонками\n",    "        new_df_to_ohe = df[enc_loans_columns + filtered_columns]\n",    "        for col in new_df_to_ohe.select_dtypes(include=['category']).columns:\n",    "            new_df_to_ohe[col] = new_df_to_ohe[col].astype('str')\n",    "        feature_columns = list(new_df_to_ohe.columns.values)\n",    "        feature_columns.remove(\"id\")\n",    "        feature_columns.remove(\"rn\")\n",    "        \n",    "        # Применяем ohe\n",    "        dummies = pd.get_dummies(new_df_to_ohe[feature_columns], columns=feature_columns)\n",    "        dummy_features = dummies.columns.values\n",    "        \n",    "        # Объединяем датафреймы \n",    "        ohe_features = pd.concat([new_df_to_ohe, dummies], axis=1)\n",    "        ohe_features = ohe_features.drop(columns=feature_columns)\n",    "        \n",    "        # Группируем полученные данные по id и sum\n",    "        ohe_features.groupby(\"id\")\n",    "        features = ohe_features.groupby(\"id\")[dummy_features].sum().reset_index(drop=False)\n",    "\n",    "        enc_loans_columns1 = ['enc_loans_account_holder_type', 'enc_loans_credit_status',\n",    "                              'enc_loans_account_cur', 'enc_loans_credit_type']\n",    "        \n",    "        # Определяем признаки для масштабирования\n",    "        df = df.drop(enc_loans_columns1, axis=1)\n",    "        df = df.drop(filtered_columns, axis=1)\n",    "        df_scale = df.drop('id', axis=1)\n",    "        \n",    "        cols_to_scale = df_scale.columns\n",    "        \n",    "        scaler = StandardScaler()\n",    "        scaler.fit(df[cols_to_scale])\n",    "        \n",    "        df[cols_to_scale] = scaler.transform(df[cols_to_scale])\n",    "        df = df.groupby('id').mean().reset_index()\n",    "        # Объединяем признаки с ohe и StandardScaler\n",    "        df = pd.merge(df, features, on='id', how='inner')\n",    "        \n",    "        #записываем подготовленные данные в файл\n",    "        if save_to_path:\n",    "            block_as_str = str(step)\n",    "            if len(block_as_str) == 1:\n",    "                block_as_str = '00' + block_as_str\n",    "            else:\n",    "                block_as_str = '0' + block_as_str\n",    "            df.to_parquet(os.path.join(save_to_path, f'processed_chunk_{block_as_str}.parquet'))\n",    "\n",    "        preprocessed_frames.append(df)\n",    "    return pd.concat(preprocessed_frames, join='inner')"   ],   "metadata": {    "collapsed": false,    "ExecuteTime": {     "end_time": "2023-12-22T08:36:40.214545Z",     "start_time": "2023-12-22T08:36:40.210652Z"    }   },   "id": "3fecd33fdf03b746"  },  {   "cell_type": "code",   "execution_count": 10,   "outputs": [    {     "data": {      "text/plain": "Transforming transactions data:   0%|          | 0/6 [00:00<?, ?it/s]",      "application/vnd.jupyter.widget-view+json": {       "version_major": 2,       "version_minor": 0,       "model_id": "2c388d18a0e44e0898cbe93a0a4b4108"      }     },     "metadata": {},     "output_type": "display_data"    },    {     "name": "stdout",     "output_type": "stream",     "text": [      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"     ]    },    {     "data": {      "text/plain": "Reading dataset with pandas:   0%|          | 0/2 [00:00<?, ?it/s]",      "application/vnd.jupyter.widget-view+json": {       "version_major": 2,       "version_minor": 0,       "model_id": "4d800e28e9ad4f0e8d88182012608bdc"      }     },     "metadata": {},     "output_type": "display_data"    },    {     "name": "stdout",     "output_type": "stream",     "text": [      "chunk_path train_data/train_data_0.pq\n",      "chunk_path train_data/train_data_1.pq\n",      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"     ]    },    {     "data": {      "text/plain": "Reading dataset with pandas:   0%|          | 0/2 [00:00<?, ?it/s]",      "application/vnd.jupyter.widget-view+json": {       "version_major": 2,       "version_minor": 0,       "model_id": "5dcb3c07f9674d669d51d259382492dd"      }     },     "metadata": {},     "output_type": "display_data"    },    {     "name": "stdout",     "output_type": "stream",     "text": [      "chunk_path train_data/train_data_10.pq\n",      "chunk_path train_data/train_data_11.pq\n",      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"     ]    },    {     "data": {      "text/plain": "Reading dataset with pandas:   0%|          | 0/2 [00:00<?, ?it/s]",      "application/vnd.jupyter.widget-view+json": {       "version_major": 2,       "version_minor": 0,       "model_id": "ee375d0de1a642c3a8d070d3dd7c7d3f"      }     },     "metadata": {},     "output_type": "display_data"    },    {     "name": "stdout",     "output_type": "stream",     "text": [      "chunk_path train_data/train_data_2.pq\n",      "chunk_path train_data/train_data_3.pq\n",      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"     ]    },    {     "data": {      "text/plain": "Reading dataset with pandas:   0%|          | 0/2 [00:00<?, ?it/s]",      "application/vnd.jupyter.widget-view+json": {       "version_major": 2,       "version_minor": 0,       "model_id": "bfcade28e66b447fa65251e225a2c2b0"      }     },     "metadata": {},     "output_type": "display_data"    },    {     "name": "stdout",     "output_type": "stream",     "text": [      "chunk_path train_data/train_data_4.pq\n",      "chunk_path train_data/train_data_5.pq\n",      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"     ]    },    {     "data": {      "text/plain": "Reading dataset with pandas:   0%|          | 0/2 [00:00<?, ?it/s]",      "application/vnd.jupyter.widget-view+json": {       "version_major": 2,       "version_minor": 0,       "model_id": "021521f267ac4ac18d373e69a5c7207f"      }     },     "metadata": {},     "output_type": "display_data"    },    {     "name": "stdout",     "output_type": "stream",     "text": [      "chunk_path train_data/train_data_6.pq\n",      "chunk_path train_data/train_data_7.pq\n",      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"     ]    },    {     "data": {      "text/plain": "Reading dataset with pandas:   0%|          | 0/2 [00:00<?, ?it/s]",      "application/vnd.jupyter.widget-view+json": {       "version_major": 2,       "version_minor": 0,       "model_id": "001330452b264c43b1f5fd595e59af7a"      }     },     "metadata": {},     "output_type": "display_data"    },    {     "name": "stdout",     "output_type": "stream",     "text": [      "chunk_path train_data/train_data_8.pq\n",      "chunk_path train_data/train_data_9.pq\n"     ]    }   ],   "source": [    "data = prepare_transactions_dataset(path, num_parts_to_preprocess_at_once=2, num_parts_total=12)"   ],   "metadata": {    "collapsed": false,    "ExecuteTime": {     "end_time": "2023-12-22T08:38:24.418381Z",     "start_time": "2023-12-22T08:36:40.216611Z"    }   },   "id": "bd0810eee29e8111"  },  {   "cell_type": "code",   "execution_count": 11,   "outputs": [    {     "data": {      "text/plain": "   id        rn  pre_since_opened  pre_since_confirmed  pre_pterm  pre_fterm  \\\n0   0 -0.227671         -0.203621            -0.168349  -0.228292  -0.179451   \n1   1  0.156628          0.375623            -0.159211  -0.315085  -0.083074   \n2   2 -0.900194         -0.163016             0.485552  -0.247278  -0.516773   \n3   3  0.252702         -0.395045            -0.225210  -0.133363  -0.111987   \n4   4 -1.092344          0.475065             0.130171  -0.816854  -0.067011   \n\n   pre_till_pclose  pre_till_fclose  pre_loans_credit_limit  \\\n0         0.842199         0.553440               -0.022794   \n1         0.585842        -0.322131               -0.273967   \n2        -0.392724         0.151151               -1.390292   \n3        -0.354133        -0.053938                0.023172   \n4        -1.164551         0.624432                0.390902   \n\n   pre_loans_next_pay_summ  ...  enc_paym_22_2  enc_paym_22_3  enc_paym_23_0  \\\n0                 0.511008  ...              0              8              2   \n1                -0.186453  ...              0             11              3   \n2                -0.776346  ...              0              2              0   \n3                 0.146662  ...              0              8              7   \n4                -1.019243  ...              0              1              0   \n\n   enc_paym_23_1  enc_paym_23_2  enc_paym_23_3  enc_paym_24_1  enc_paym_24_2  \\\n0              0              0              8              0              0   \n1              0              0             11              3              0   \n2              1              0              2              0              0   \n3              0              0              8              5              0   \n4              0              0              1              0              0   \n\n   enc_paym_24_3  enc_paym_24_4  \n0              0             10  \n1              0             11  \n2              0              3  \n3              0             10  \n4              0              1  \n\n[5 rows x 158 columns]",      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>rn</th>\n      <th>pre_since_opened</th>\n      <th>pre_since_confirmed</th>\n      <th>pre_pterm</th>\n      <th>pre_fterm</th>\n      <th>pre_till_pclose</th>\n      <th>pre_till_fclose</th>\n      <th>pre_loans_credit_limit</th>\n      <th>pre_loans_next_pay_summ</th>\n      <th>...</th>\n      <th>enc_paym_22_2</th>\n      <th>enc_paym_22_3</th>\n      <th>enc_paym_23_0</th>\n      <th>enc_paym_23_1</th>\n      <th>enc_paym_23_2</th>\n      <th>enc_paym_23_3</th>\n      <th>enc_paym_24_1</th>\n      <th>enc_paym_24_2</th>\n      <th>enc_paym_24_3</th>\n      <th>enc_paym_24_4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-0.227671</td>\n      <td>-0.203621</td>\n      <td>-0.168349</td>\n      <td>-0.228292</td>\n      <td>-0.179451</td>\n      <td>0.842199</td>\n      <td>0.553440</td>\n      <td>-0.022794</td>\n      <td>0.511008</td>\n      <td>...</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.156628</td>\n      <td>0.375623</td>\n      <td>-0.159211</td>\n      <td>-0.315085</td>\n      <td>-0.083074</td>\n      <td>0.585842</td>\n      <td>-0.322131</td>\n      <td>-0.273967</td>\n      <td>-0.186453</td>\n      <td>...</td>\n      <td>0</td>\n      <td>11</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>-0.900194</td>\n      <td>-0.163016</td>\n      <td>0.485552</td>\n      <td>-0.247278</td>\n      <td>-0.516773</td>\n      <td>-0.392724</td>\n      <td>0.151151</td>\n      <td>-1.390292</td>\n      <td>-0.776346</td>\n      <td>...</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.252702</td>\n      <td>-0.395045</td>\n      <td>-0.225210</td>\n      <td>-0.133363</td>\n      <td>-0.111987</td>\n      <td>-0.354133</td>\n      <td>-0.053938</td>\n      <td>0.023172</td>\n      <td>0.146662</td>\n      <td>...</td>\n      <td>0</td>\n      <td>8</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>-1.092344</td>\n      <td>0.475065</td>\n      <td>0.130171</td>\n      <td>-0.816854</td>\n      <td>-0.067011</td>\n      <td>-1.164551</td>\n      <td>0.624432</td>\n      <td>0.390902</td>\n      <td>-1.019243</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 158 columns</p>\n</div>"     },     "execution_count": 11,     "metadata": {},     "output_type": "execute_result"    }   ],   "source": [    "data.head()"   ],   "metadata": {    "collapsed": false,    "ExecuteTime": {     "end_time": "2023-12-22T08:38:24.436054Z",     "start_time": "2023-12-22T08:38:24.416687Z"    }   },   "id": "81557a0334f81cdb"  },  {   "cell_type": "code",   "execution_count": 12,   "outputs": [    {     "data": {      "text/plain": "0"     },     "execution_count": 12,     "metadata": {},     "output_type": "execute_result"    }   ],   "source": [    "data.isna().sum().sum()"   ],   "metadata": {    "collapsed": false,    "ExecuteTime": {     "end_time": "2023-12-22T08:38:24.853439Z",     "start_time": "2023-12-22T08:38:24.426185Z"    }   },   "id": "d0a0577354157fab"  },  {   "cell_type": "code",   "execution_count": 13,   "outputs": [    {     "name": "stdout",     "output_type": "stream",     "text": [      "Объединение файлов завершено.\n",      "Сохранение файла завершено.\n"     ]    }   ],   "source": [    "df = pd.merge(data, target_df[['id', 'flag']], on='id', how='left')\n",    "df = df.drop('id', axis=1)\n",    "print(f\"Объединение файлов завершено.\")\n",    "parquet_file_path = 'data_part_ohe_with_std.parquet'\n",    "# Преобразуем DataFrame в таблицу pyarrow\n",    "table = pa.Table.from_pandas(df)\n",    "\n",    "# Сохраняем таблицу в формате Parquet\n",    "pq.write_table(table, parquet_file_path)\n",    "print(f\"Сохранение файла завершено.\")"   ],   "metadata": {    "collapsed": false,    "ExecuteTime": {     "end_time": "2023-12-22T08:38:35.490183Z",     "start_time": "2023-12-22T08:38:24.850611Z"    }   },   "id": "e187a99687eaa9b2"  },  {   "cell_type": "code",   "execution_count": 15,   "outputs": [    {     "data": {      "text/plain": "0          0.343826\n1         -0.839046\n2         -0.941017\n3          0.486586\n4          0.486586\n             ...   \n2999995   -0.121921\n2999996    0.465294\n2999997    0.367425\n2999998    0.269555\n2999999    0.282605\nName: total_delinquencies, Length: 3000000, dtype: float64"     },     "execution_count": 15,     "metadata": {},     "output_type": "execute_result"    }   ],   "source": [    "df.total_delinquencies"   ],   "metadata": {    "collapsed": false,    "ExecuteTime": {     "end_time": "2023-12-22T08:39:56.412807Z",     "start_time": "2023-12-22T08:39:56.409212Z"    }   },   "id": "3494fd2e2d716e71"  },  {   "cell_type": "code",   "execution_count": null,   "outputs": [],   "source": [],   "metadata": {    "collapsed": false   },   "id": "1583a615ffc2b1ce"  } ], "metadata": {  "kernelspec": {   "display_name": "Python 3",   "language": "python",   "name": "python3"  },  "language_info": {   "codemirror_mode": {    "name": "ipython",    "version": 2   },   "file_extension": ".py",   "mimetype": "text/x-python",   "name": "python",   "nbconvert_exporter": "python",   "pygments_lexer": "ipython2",   "version": "2.7.6"  } }, "nbformat": 4, "nbformat_minor": 5}