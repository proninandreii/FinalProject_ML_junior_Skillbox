import dillfrom sklearn.preprocessing import FunctionTransformerfrom sklearn.pipeline import Pipelinefrom catboost import CatBoostClassifierfrom datetime import datetimefrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import roc_auc_scoreimport matplotlib.pyplot as pltimport osimport pandas as pdimport tqdmimport timeimport warningswarnings.filterwarnings('ignore')plt.style.use('fivethirtyeight')target_path = "train_target.csv"path = 'train_data'target_df = pd.read_csv(target_path)print(f"Обработка файла завершена.")def read_parquet_dataset_from_local(path_to_dataset: str, start_from: int = 0,                                    num_parts_to_read: int = 2, columns=None, verbose=False) -> pd.DataFrame:    """    читает num_parts_to_read партиций, преобразовывает их к pd.DataFrame и возвращает    :param path_to_dataset: путь до директории с партициями    :param start_from: номер партиции, с которой нужно начать чтение    :param num_parts_to_read: количество партиций, которые требуется прочитать    :param columns: список колонок, которые нужно прочитать из партиции    :return: pd.DataFrame    """    res = []    dataset_paths = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset)                            if filename.startswith('train')])    print(dataset_paths)    start_from = max(0, start_from)    chunks = dataset_paths[start_from: start_from + num_parts_to_read]    if verbose:        print('Reading chunks:\n')        for chunk in chunks:            print(chunk)    for chunk_path in tqdm.tqdm_notebook(chunks, desc="Reading dataset with pandas"):        print('chunk_path', chunk_path)        chunk = pd.read_parquet(chunk_path,columns=columns)        res.append(chunk)    return pd.concat(res).reset_index(drop=True)def prepare_transactions_dataset(path_to_dataset: str, num_parts_to_preprocess_at_once: int = 1, num_parts_total: int=50,                                 save_to_path=None, verbose: bool=False):    """    возвращает готовый pd.DataFrame с признаками, на которых можно учить модель для целевой задачи    path_to_dataset: str        путь до датасета с партициями    num_parts_to_preprocess_at_once: int        количество партиций, которые будут одновременно держаться и обрабатываться в памяти    num_parts_total: int        общее количество партиций, которые нужно обработать    save_to_path: str        путь до папки, в которой будет сохранён каждый обработанный блок в .parquet-формате; если None, то не будет сохранён    verbose: bool        логирует каждую обрабатываемую часть данных    """    preprocessed_frames = []    for step in tqdm.tqdm_notebook(range(0, num_parts_total, num_parts_to_preprocess_at_once),                                   desc="Transforming transactions data"):        data_frame = read_parquet_dataset_from_local(path_to_dataset, step, num_parts_to_preprocess_at_once,                                                     verbose=verbose)        #здесь должен быть препроцессинг данных        data_frame['no_delinquencies'] = (data_frame['is_zero_loans530'].astype(bool) & data_frame['is_zero_loans3060'].astype(bool) &                                          data_frame['is_zero_loans90'].astype(bool) & data_frame['is_zero_loans6090'].astype(bool) &                                          data_frame['is_zero_loans5'].astype(bool)).astype(int)        data_frame['total_delinquencies'] = (data_frame['is_zero_loans530'] + data_frame['is_zero_loans3060'] + data_frame['is_zero_loans5'] +                                             data_frame['is_zero_loans90'] + data_frame['is_zero_loans6090'])        binarized_columns = ['is_zero_loans5', 'is_zero_loans530', 'is_zero_loans3060', 'is_zero_loans6090',                             'is_zero_loans90', 'is_zero_util', 'is_zero_over2limit',                             'is_zero_maxover2limit', 'pclose_flag', 'fclose_flag']        df_second = data_frame[['id'] + binarized_columns].copy()        df_first = data_frame.drop(binarized_columns, axis=1)        feature_columns = list(df_first.columns.values)        feature_columns.remove("id")        feature_columns.remove("rn")        dummies = pd.get_dummies(df_first[feature_columns], columns=feature_columns)        dummy_features = dummies.columns.values        ohe_features = pd.concat([df_first, dummies], axis=1)        ohe_features = ohe_features.drop(columns=feature_columns)        ohe_features.groupby("id")        features = ohe_features.groupby("id")[dummy_features].sum().reset_index(drop=False)        df_second = df_second.groupby('id').sum().reset_index(drop=False)        df_result = pd.merge(features, df_second, on='id')        df_result = pd.merge(df_result, target_df[['id', 'flag']], on='id', how='left')        #записываем подготовленные данные в файл        preprocessed_frames.append(df_result)    return pd.concat(preprocessed_frames, join='inner', ignore_index=True)def drop_columns(data):    return data.drop(columns=['id'])def train_and_evaluate_model():    start_time = time.time()    # Задаем лучшую найденную модель    model = CatBoostClassifier(random_state=42, l2_leaf_reg=4, learning_rate=0.1, depth=6, n_estimators=2000, bootstrap_type='Bernoulli')    # Задаем пайплайн    pipe = Pipeline(steps=[        ('drop', FunctionTransformer(drop_columns)),        ('model', model)    ])    # Вызываем функцию распаковки файлов с данными    data = prepare_transactions_dataset(path, num_parts_to_preprocess_at_once=2, num_parts_total=4)    # Делим данные    X = data.drop("flag", axis=1)    y = data["flag"]    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)    # Обучаем модель на тренировочных данных по ТЗ    pipe.fit(X_train, y_train)    # Предсказание модели на тесте сохраняем в отдельный файл    y_pred = pipe.predict_proba(X_test)[:, 1]    predictions_df = pd.DataFrame({        'ID': range(1, len(y_pred) + 1),        'Prediction': y_pred    })    roc_auc = roc_auc_score(y_test, y_pred)    print('ROC AUC test: ', roc_auc)    # Сохраняем DataFrame в CSV-файл    predictions_df.to_csv('predictions.csv', index=False)    # Кросс-валидацию не добавляем, работу на кросс-валидации можно посмотреть в конце Modelling_FULL_OHE    # Учим модель на всех данных    pipe.fit(X, y)    end_time = time.time()    training_time = end_time - start_time    print(f"Training time: {training_time:.2f} seconds")    # Сохраняем в pkl файл по ТЗ    with open('final_model.pkl', 'wb') as file:        dill.dump({            'model': pipe,            'metadata': {                'name': 'Final project ML',                'author': 'Andrey Pronin',                'version': 1,                'date': datetime.now(),                'type': type(pipe.named_steps['model']).__name__,                'metric': roc_auc,                'training_time': training_time            }        }, file)if __name__ == "__main__":    train_and_evaluate_model()#%%#%%#%%